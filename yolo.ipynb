{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0be136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Path ì •ë¦¬\n",
    "DATA_DIR = Path(\"/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/data\")\n",
    "BASE_DIR = Path(\"/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_yolo_finetuning/\")\n",
    "CSV_PATH = DATA_DIR / \"metadata.csv\"\n",
    "\n",
    "# CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "target = 1                              # ì›í•˜ëŠ” í–‰ ì¸ë±ìŠ¤ ì„¤ì •\n",
    "\n",
    "COMMON_PATH = df.loc[target,\"common_path\"]   # COMMON_PATH ì¶”ì¶œ\n",
    "VIDEO_PTH = df.loc[target,\"video_path\"]\n",
    "FRAME_DIR = DATA_DIR / \"1_FRAME\" / COMMON_PATH          # í”„ë ˆì„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "KPT_DIR = DATA_DIR / \"2_KEYPOINTS\" / COMMON_PATH        # í‚¤í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "MP4_DIR = DATA_DIR / \"3_MP4\" / f\"{COMMON_PATH}.mp4\"              # MP4 ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •    \n",
    "INTERP_DIR = DATA_DIR / \"4_INTERP_DATA\" /COMMON_PATH    # ë³´ê°„ ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "YOLO_DIR = DATA_DIR / \"5_YOLO_TXT\" / COMMON_PATH     # YOLO TXT ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •\n",
    "\n",
    "OUTPUT_PATH = DATA_DIR / \"test\"\n",
    "\n",
    "# ê²½ë¡œ ì¶œë ¥\n",
    "print(\" BASE_DIR:\", BASE_DIR,\"\\n FRAME_DIR:\", FRAME_DIR, \"\\n KPT_DIR:\", KPT_DIR, \"\\n MP4_DIR:\", MP4_DIR, \"\\n INTERP_DIR:\", INTERP_DIR)\n",
    "print(\"IS_TRAIN:\", df.loc[target,\"is_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382c57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(str(BASE_DIR))\n",
    "from funcs.data_utils import convert_json_to_yolo_kpt_fixed, create_yolo_dataset_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9292cb",
   "metadata": {},
   "source": [
    "# JSON2TXT\n",
    "íŒŒì´ì¬ íŒŒì¼ runnerë¡œ ë³€ê²½ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ==========================================\n",
    "# # 1. ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "# # ==========================================\n",
    "# DATA_DIR = Path(\"/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/data\")\n",
    "# CSV_PATH = DATA_DIR / \"metadata.csv\"\n",
    "\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "# target_row = df[df['is_train'] == True].iloc[1]\n",
    "\n",
    "# COMMON_PATH = target_row['common_path']\n",
    "# FRAME_DIR = DATA_DIR / \"1_FRAME\" / COMMON_PATH\n",
    "# INTERP_DIR = DATA_DIR / \"4_INTERP_DATA\" / COMMON_PATH\n",
    "# YOLO_DIR = DATA_DIR / \"5_YOLO_TXT\" / COMMON_PATH\n",
    "\n",
    "# print(f\"ğŸ§ª [TEST START] íƒ€ê²Ÿ ê²½ë¡œ: {COMMON_PATH}\")\n",
    "# print(f\"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {YOLO_DIR}\")\n",
    "\n",
    "# # ==========================================\n",
    "# # 3. ì‹¤í–‰\n",
    "# # ==========================================\n",
    "\n",
    "# YOLO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# img_files = list(FRAME_DIR.glob(\"*.jpg\")) + list(FRAME_DIR.glob(\"*.png\"))\n",
    "\n",
    "# if not img_files:\n",
    "#     print(\"âŒ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "# else:\n",
    "#     sample_img = cv2.imread(str(img_files[0]))\n",
    "#     H, W = sample_img.shape[:2]\n",
    "#     print(f\"ğŸ“ Image Size: W={W}, H={H}\")\n",
    "\n",
    "#     json_files = list(INTERP_DIR.glob(\"*.json\"))\n",
    "#     success_count = 0\n",
    "    \n",
    "#     print(f\"ğŸš€ ë³€í™˜ ì‹œì‘ ({len(json_files)}ê°œ íŒŒì¼)...\")\n",
    "\n",
    "#     for json_file in tqdm(json_files):\n",
    "#         txt_file = YOLO_DIR / f\"{json_file.stem}.txt\"\n",
    "#         if convert_json_to_yolo_kpt_fixed(json_file, txt_file, W, H):\n",
    "#             success_count += 1\n",
    "\n",
    "#     print(f\"\\nâœ… ì™„ë£Œ: {success_count}ê°œ ìƒì„±ë¨.\")\n",
    "    \n",
    "#     if success_count > 0:\n",
    "#         sample_txt = list(YOLO_DIR.glob(\"*.txt\"))[0]\n",
    "#         with open(sample_txt, 'r') as f:\n",
    "#             print(f\"\\nğŸ“„ [ìƒ˜í”Œ ë°ì´í„°] {sample_txt.name}\\n{f.read()}\")\n",
    "#     else:\n",
    "#         print(\"âš ï¸ ìƒì„±ëœ TXT íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1102c8a",
   "metadata": {},
   "source": [
    "## ì œëŒ€ë¡œ ìƒì„±ëëŠ”ì§€ í™•ì¸í•˜ëŠ” ì½”ë“œ\n",
    "í™•ì¸ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# # ==========================================\n",
    "# # âš™ï¸ ì„¤ì • ë° ìŠ¤ì¼ˆë ˆí†¤ ì •ì˜\n",
    "# # ==========================================\n",
    "# # ìš°ë¦¬ê°€ ì¶”ì¶œí•œ 5~16ë²ˆ í‚¤í¬ì¸íŠ¸ (ì´ 12ê°œ)ì— ëŒ€í•œ ë‚´ë¶€ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "# # (Original ID -> Local List Index)\n",
    "# # 5:L-Sh, 6:R-Sh, 7:L-Elb, 8:R-Elb, 9:L-Wri, 10:R-Wri\n",
    "# # 11:L-Hip, 12:R-Hip, 13:L-Knee, 14:R-Knee, 15:L-Ank, 16:R-Ank\n",
    "\n",
    "# # ì‹œê°í™”ë¥¼ ìœ„í•œ ì—°ê²° ì •ë³´ (Local Index ê¸°ì¤€ 0~11)\n",
    "# SKELETON_CONNECTIONS = [\n",
    "#     (0, 1),   # Shoulders\n",
    "#     (0, 2), (2, 4), # Left Arm\n",
    "#     (1, 3), (3, 5), # Right Arm\n",
    "#     (0, 6), (1, 7), # Torso (Shoulder -> Hip)\n",
    "#     (6, 7),   # Hips\n",
    "#     (6, 8), (8, 10), # Left Leg\n",
    "#     (7, 9), (9, 11)  # Right Leg\n",
    "# ]\n",
    "\n",
    "# def visualize_yolo_overlay(img_path, txt_path):\n",
    "#     \"\"\"\n",
    "#     ì´ë¯¸ì§€ì™€ YOLO txt íŒŒì¼ì„ ì½ì–´ BBoxì™€ Keypointsë¥¼ ê·¸ë¦½ë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     # 1. ì´ë¯¸ì§€ ë¡œë“œ\n",
    "#     img = cv2.imread(str(img_path))\n",
    "#     if img is None:\n",
    "#         print(f\"âŒ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}\")\n",
    "#         return\n",
    "    \n",
    "#     # BGR -> RGB ë³€í™˜ (Matplotlibìš©)\n",
    "#     img_vis = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     H, W = img_vis.shape[:2]\n",
    "\n",
    "#     # 2. TXT íŒŒì¼ ì½ê¸°\n",
    "#     if not txt_path.exists():\n",
    "#         print(f\"âŒ ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {txt_path}\")\n",
    "#         return\n",
    "\n",
    "#     with open(txt_path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     print(f\"ğŸ” ê°ì§€ëœ ê°ì²´ ìˆ˜: {len(lines)}\")\n",
    "\n",
    "#     # 3. íŒŒì‹± ë° ê·¸ë¦¬ê¸°\n",
    "#     for line in lines:\n",
    "#         data = list(map(float, line.strip().split()))\n",
    "        \n",
    "#         # í¬ë§·: <class> <cx> <cy> <w> <h> <kpt1_x> <kpt1_y> <kpt1_v> ...\n",
    "#         cls_id = int(data[0])\n",
    "#         cx, cy, w, h = data[1:5]\n",
    "#         kpts = data[5:]\n",
    "\n",
    "#         # --- A. Bounding Box ê·¸ë¦¬ê¸° ---\n",
    "#         # Normalized Center XYWH -> Pixel Top-Left XY\n",
    "#         x1 = int((cx - w / 2) * W)\n",
    "#         y1 = int((cy - h / 2) * H)\n",
    "#         x2 = int((cx + w / 2) * W)\n",
    "#         y2 = int((cy + h / 2) * H)\n",
    "\n",
    "#         # ë…¹ìƒ‰ ë°•ìŠ¤ (ë‘ê»˜ 2)\n",
    "#         cv2.rectangle(img_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv2.putText(img_vis, f\"Person\", (x1, y1 - 10), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "#         # --- B. Keypoints ê·¸ë¦¬ê¸° ---\n",
    "#         # kpts ë¦¬ìŠ¤íŠ¸ë¥¼ (x, y, v) ë¬¶ìŒìœ¼ë¡œ ë³€í™˜\n",
    "#         points = []\n",
    "#         for i in range(0, len(kpts), 3):\n",
    "#             kx, ky, kv = kpts[i], kpts[i+1], kpts[i+2]\n",
    "            \n",
    "#             # í”½ì…€ ì¢Œí‘œ ë³€í™˜\n",
    "#             px, py = int(kx * W), int(ky * H)\n",
    "#             points.append((px, py))\n",
    "\n",
    "#             # ì  ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰, ê°€ì‹œì„± ìˆëŠ” ê²½ìš°ë§Œ)\n",
    "#             if kx > 0 and ky > 0: # í˜¹ì€ kv > 0\n",
    "#                 cv2.circle(img_vis, (px, py), 4, (255, 0, 0), -1) # Red Dot\n",
    "\n",
    "#         # --- C. Skeleton ì—°ê²°ì„  ê·¸ë¦¬ê¸° ---\n",
    "#         for i, j in SKELETON_CONNECTIONS:\n",
    "#             # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬ ë° ì¢Œí‘œ ìœ íš¨ì„± ì²´í¬\n",
    "#             if i < len(points) and j < len(points):\n",
    "#                 pt1 = points[i]\n",
    "#                 pt2 = points[j]\n",
    "                \n",
    "#                 # ë‘˜ ë‹¤ (0,0)ì´ ì•„ë‹ ë•Œë§Œ ì„  ê¸‹ê¸°\n",
    "#                 if (pt1[0] > 0 and pt1[1] > 0) and (pt2[0] > 0 and pt2[1] > 0):\n",
    "#                     cv2.line(img_vis, pt1, pt2, (0, 255, 255), 2) # Yellow Line\n",
    "\n",
    "#     # 4. ì¶œë ¥ (Matplotlib)\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     plt.imshow(img_vis)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Overlay Result: {txt_path.name}\")\n",
    "#     plt.show()\n",
    "\n",
    "# # ==========================================\n",
    "# # ğŸš€ ì‹¤í–‰\n",
    "# # ==========================================\n",
    "# # ì•ì„œ ìƒì„±ëœ íŒŒì¼ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "# # YOLO_DIR ë³€ìˆ˜ê°€ ì´ì „ ì½”ë“œì—ì„œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "# # ì˜ˆì‹œ: í´ë” ë‚´ ì²« ë²ˆì§¸ txt íŒŒì¼ê³¼ ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ ì°¾ê¸°\n",
    "# txt_files = list(YOLO_DIR.glob(\"*.txt\"))\n",
    "\n",
    "# if txt_files:\n",
    "#     test_txt = txt_files[200]\n",
    "#     # ì´ë¯¸ì§€ ì´ë¦„ì€ txt íŒŒì¼ëª…ê³¼ ê°™ë‹¤ê³  ê°€ì • (í™•ì¥ìë§Œ ë‹¤ë¦„)\n",
    "#     # ì´ë¯¸ì§€ í™•ì¥ìë¥¼ ëª¨ë¥´ë‹ˆ jpg, png ì‹œë„\n",
    "#     test_img = FRAME_DIR / f\"{test_txt.stem}.jpg\"\n",
    "#     if not test_img.exists():\n",
    "#         test_img = FRAME_DIR / f\"{test_txt.stem}.png\"\n",
    "\n",
    "#     print(f\"ğŸ–¼ï¸ ì´ë¯¸ì§€: {test_img}\")\n",
    "#     print(f\"ğŸ“ ë¼ë²¨: {test_txt}\")\n",
    "    \n",
    "#     visualize_yolo_overlay(test_img, test_txt)\n",
    "# else:\n",
    "#     print(\"âŒ í™•ì¸í•  TXT íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e8011",
   "metadata": {},
   "source": [
    "# datasset êµ¬ì„± \n",
    "symlink ì‚¬ìš©í•´ì„œ ìš©ëŸ‰ì„ ì¤„ì„. dataset êµ¬ì¡°ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì½”ë“œ  \n",
    "funcsì— `create_yolo_dataset_structure(df, dataset_dir, data_dir, step=30)` í•¨ìˆ˜ë¡œ ìƒì„±í–ˆìŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import yaml\n",
    "# import os\n",
    "# import shutil\n",
    "# from pathlib import Path\n",
    "\n",
    "# # ==========================================\n",
    "# # 1. ì„¤ì • ë° ê²½ë¡œ\n",
    "# # ==========================================\n",
    "# # ì›ë³¸ ë°ì´í„° ê²½ë¡œ (ìš”ì²­í•˜ì‹  ê²½ë¡œ ê³ ì •)\n",
    "# DATA_DIR = Path(\"/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/data\")\n",
    "# CSV_PATH = DATA_DIR / \"metadata.csv\"\n",
    "\n",
    "# # í…ŒìŠ¤íŠ¸ìš© ê²°ê³¼ê°€ ì €ì¥ë  ê²½ë¡œ\n",
    "# TEST_DATASET_DIR = DATA_DIR / \"6_YOLO_TRAINING_DATA/v1.0\"\n",
    "\n",
    "# # ==========================================\n",
    "# # 3. ë©”ì¸ ì‹¤í–‰ë¶€ (2ê°œ ìƒ˜í”Œ ì¶”ì¶œ ë° ì‹¤í–‰)\n",
    "# # ==========================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ\n",
    "#     if not CSV_PATH.exists():\n",
    "#         print(f\"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}\")\n",
    "#     else:\n",
    "#         full_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "#         # 2. í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (Train 1ê°œ + Val 1ê°œ)\n",
    "#         try:\n",
    "#             sample_rows = []\n",
    "            \n",
    "#             # Train ìƒ˜í”Œ (ì²« ë²ˆì§¸ í–‰)\n",
    "#             train_rows = full_df[full_df['is_train'] == True]\n",
    "#             if not train_rows.empty:\n",
    "#                 sample_rows.append(train_rows.iloc[0])\n",
    "            \n",
    "#             # Val ìƒ˜í”Œ (ì²« ë²ˆì§¸ í–‰)\n",
    "#             if 'is_val' in full_df.columns:\n",
    "#                 val_rows = full_df[full_df['is_val'] == True]\n",
    "#                 if not val_rows.empty:\n",
    "#                     sample_rows.append(val_rows.iloc[0])\n",
    "#             else:\n",
    "#                 print(\"âš ï¸ 'is_val' ì»¬ëŸ¼ì´ ì—†ì–´ Train ë°ì´í„°ë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "#             if sample_rows:\n",
    "#                 test_df = pd.DataFrame(sample_rows)\n",
    "#                 print(f\"ğŸ§ª í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: ì´ {len(test_df)}ê°œ í´ë”\")\n",
    "                \n",
    "#                 # 3. í•¨ìˆ˜ ì‹¤í–‰\n",
    "#                 create_yolo_dataset_structure(test_df, TEST_DATASET_DIR, DATA_DIR, step=30)\n",
    "                \n",
    "#                 # 4. ê²°ê³¼ í™•ì¸\n",
    "#                 print(\"\\nğŸ“‚ ìƒì„±ëœ í´ë” êµ¬ì¡° í™•ì¸:\")\n",
    "#                 for root, dirs, files in os.walk(TEST_DATASET_DIR):\n",
    "#                     level = root.replace(str(TEST_DATASET_DIR), '').count(os.sep)\n",
    "#                     indent = ' ' * 4 * (level)\n",
    "#                     print(f\"{indent}{os.path.basename(root)}/\")\n",
    "#                     for f in files[:3]: \n",
    "#                         print(f\"{indent}    {f}\")\n",
    "#                     if len(files) > 3:\n",
    "#                         print(f\"{indent}    ... ({len(files)-3} more files)\")\n",
    "#             else:\n",
    "#                 print(\"âŒ í…ŒìŠ¤íŠ¸í•  ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c8430",
   "metadata": {},
   "source": [
    "# YOLO finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import yaml\n",
    "# from pathlib import Path\n",
    "# from ultralytics import YOLO\n",
    "# import wandb\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„\n",
    "# # ---------------------------------------------------------\n",
    "# ENV_PATH = \"/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_yolo_finetuning/.env\"\n",
    "# load_dotenv(ENV_PATH)\n",
    "# WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# CONFIG_PATH = \"/workspace/nas203/ds_RehabilitationMedicineData/IDs/tojihoo/ASAN_01_mini_yolo_finetuning/exp_v1.yaml\"\n",
    "# with open(CONFIG_PATH, 'r') as f:\n",
    "#     cfg = yaml.safe_load(f)\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # ğŸ› ï¸ í•¨ìˆ˜: ê²½ë¡œ ìœ ë™ì„± í•´ê²° & ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "# # ---------------------------------------------------------\n",
    "# def update_data_yaml_and_get_info(yaml_path):\n",
    "#     path_obj = Path(yaml_path)\n",
    "#     with open(path_obj, 'r') as f:\n",
    "#         data_cfg = yaml.safe_load(f)\n",
    "    \n",
    "#     current_data_dir = path_obj.parent.resolve()\n",
    "#     print(f\"ğŸ”„ ë°ì´í„° ê²½ë¡œ ê°±ì‹ : {data_cfg.get('path')} -> {current_data_dir}\")\n",
    "#     data_cfg['path'] = str(current_data_dir)\n",
    "    \n",
    "#     step_info = data_cfg.get('sampling_step', 'Unknown')\n",
    "#     print(f\"â„¹ï¸ ë°ì´í„°ì…‹ Sampling Step: {step_info}\")\n",
    "\n",
    "#     with open(path_obj, 'w') as f:\n",
    "#         yaml.dump(data_cfg, f, sort_keys=False)\n",
    "#     return str(path_obj), step_info\n",
    "\n",
    "# target_data_yaml = cfg['data']['config_path']\n",
    "# fixed_data_yaml, dataset_step = update_data_yaml_and_get_info(target_data_yaml)\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # ğŸš€ WandB ì´ˆê¸°í™”\n",
    "# # ---------------------------------------------------------\n",
    "# PROJECT = cfg['project_name']\n",
    "# RUN_NAME = cfg['run_name']\n",
    "\n",
    "# if cfg['logging']['use_wandb'] and WANDB_API_KEY:\n",
    "#     try:\n",
    "#         wandb.login(key=WANDB_API_KEY)\n",
    "#         wandb_config = cfg.copy()\n",
    "#         wandb_config['dataset'] = {'sampling_step': dataset_step, 'yaml_path': fixed_data_yaml}\n",
    "\n",
    "#         wandb.init(\n",
    "#             project=PROJECT,\n",
    "#             name=RUN_NAME,\n",
    "#             config=wandb_config,\n",
    "#             resume=\"allow\",\n",
    "#             dir=cfg['output']['base_dir']\n",
    "#         )\n",
    "#         print(f\"âœ… WandB ì´ˆê¸°í™” ì„±ê³µ (Pose Task | Step: {dataset_step})\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # ğŸ› ï¸ [í•µì‹¬] ì»¤ìŠ¤í…€ WandB ì½œë°± (Pose Metrics í¬í•¨)\n",
    "# # ---------------------------------------------------------\n",
    "# def on_train_epoch_end(trainer):\n",
    "#     \"\"\"\n",
    "#     ë§¤ ì—í­ ì¢…ë£Œ ì‹œ ì‹¤í–‰. Pose Loss, Box Loss, mAP ë“±ì„ ëª¨ë‘ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     if wandb.run:\n",
    "#         # trainer.metrics ì•ˆì—ëŠ” 'pose_loss', 'box_loss' ë“±ì´ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "#         wandb.log(trainer.metrics)\n",
    "#         wandb.log({\"epoch\": trainer.epoch + 1})\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # ğŸ¤– ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ë¡œë“œ & ì´ì–´í•˜ê¸° (Pose Model ì „ìš©)\n",
    "# # ---------------------------------------------------------\n",
    "# CHECKPOINT_DIR = os.path.join(cfg['output']['base_dir'], RUN_NAME, 'weights')\n",
    "# LAST_PT_PATH = os.path.join(CHECKPOINT_DIR, 'last.pt')\n",
    "# BASE_MODEL_PATH = cfg['model']['base_path']\n",
    "\n",
    "# resume_status = False\n",
    "\n",
    "# # 1. ì´ì–´í•˜ê¸° (last.pt) ì²´í¬\n",
    "# if os.path.exists(LAST_PT_PATH):\n",
    "#     print(f\"ğŸ”„ [Resume] ì´ì „ í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: {LAST_PT_PATH}\")\n",
    "#     model = YOLO(LAST_PT_PATH)\n",
    "#     resume_status = True\n",
    "\n",
    "# # 2. ì²˜ìŒ ì‹œì‘ (ì„¤ì • íŒŒì¼ì˜ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©)\n",
    "# elif os.path.exists(BASE_MODEL_PATH):\n",
    "#     print(f\"ğŸ†• [Start] ì„¤ì •ëœ Pose ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {BASE_MODEL_PATH}\")\n",
    "#     model = YOLO(BASE_MODEL_PATH)\n",
    "#     resume_status = False\n",
    "\n",
    "# # 3. íŒŒì¼ ì—†ìŒ (ìë™ ë‹¤ìš´ë¡œë“œ - Pose ë²„ì „ ëª…ì‹œ)\n",
    "# else:\n",
    "#     print(f\"âš ï¸ [Download] ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ 'yolo11n-pose.pt'ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "#     # ì‚¬ìš©ìê°€ ì‹¤ìˆ˜ë¡œ ì¼ë°˜ ëª¨ë¸ì„ ì ì—ˆë”ë¼ë„, íŒŒì¼ì´ ì—†ìœ¼ë©´ í™•ì‹¤í•˜ê²Œ pose ëª¨ë¸ì„ ë°›ë„ë¡ ì²˜ë¦¬\n",
    "#     model = YOLO(\"yolo11n-pose.pt\") \n",
    "#     resume_status = False\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # ğŸ”— ì½œë°± ë“±ë¡ ë° í•™ìŠµ ì‹œì‘\n",
    "# # ---------------------------------------------------------\n",
    "# # ì»¤ìŠ¤í…€ ì½œë°± ë“±ë¡\n",
    "# model.add_callback(\"on_train_epoch_end\", on_train_epoch_end)\n",
    "# print(\"âœ… ì»¤ìŠ¤í…€ WandB ì½œë°± ë“±ë¡ ì™„ë£Œ\")\n",
    "\n",
    "# print(f\"\\nğŸ”¥ Pose Estimation í•™ìŠµ ì‹œì‘: {RUN_NAME} (Resume: {resume_status})\")\n",
    "\n",
    "# model.train(\n",
    "#     data=fixed_data_yaml,\n",
    "#     project=cfg['output']['base_dir'], \n",
    "#     name=RUN_NAME,\n",
    "#     resume=resume_status,\n",
    "#     plots=True,\n",
    "#     **cfg['train'] \n",
    "# )\n",
    "\n",
    "# if wandb.run:\n",
    "#     wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
